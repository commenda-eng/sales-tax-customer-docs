---
title: Debugging
description: Debug issues in the Address API
---

This guide explains how to debug issues in the Address API using CloudWatch logs and other AWS tools.

## CloudWatch logs

The Address API logs all requests and errors to CloudWatch Logs.

### Log groups

| Environment | Log group |
|-------------|-----------|
| Staging | `/ecs/staging/address-api` |
| Production | `/ecs/prod/address-api` |

### Viewing logs

#### Using AWS Console

1. Go to **CloudWatch** â†’ **Log groups**
2. Search for `/ecs/staging/address-api` or `/ecs/prod/address-api`
3. Click on the log group
4. Select a log stream (one per ECS task)
5. View logs

#### Using AWS CLI

```bash
# Tail logs in real-time
aws logs tail /ecs/staging/address-api --follow --region ap-south-1

# View recent logs
aws logs tail /ecs/staging/address-api --since 1h --region ap-south-1

# Filter logs by pattern
aws logs filter-log-events \\\n  --log-group-name /ecs/staging/address-api \\\n  --filter-pattern "ERROR" \\\n  --region ap-south-1
```

### Log format

The Address API uses structured JSON logging with zerolog:

```json
{
  "level": "info",
  "method": "POST",
  "path": "/api/v1/geoencode",
  "status": 200,
  "remote_ip": "10.0.1.45",
  "time": 1734518400,
  "message": "request"
}
```

### Log levels

| Level | Description | Example |
|-------|-------------|---------|
| `info` | Normal operations | Request logs, server startup |
| `warn` | Warnings | Deprecated features, slow queries |
| `error` | Errors | Failed requests, database errors |
| `fatal` | Fatal errors | Server startup failures |

### Common log patterns

#### Successful request

```json
{
  "level": "info",
  "method": "POST",
  "path": "/api/v1/geoencode",
  "status": 200,
  "remote_ip": "10.0.1.45",
  "time": 1734518400,
  "message": "request"
}
```

#### Failed request

```json
{
  "level": "error",
  "method": "POST",
  "path": "/api/v1/geoencode",
  "status": 500,
  "error": "geocoding failed",
  "remote_ip": "10.0.1.45",
  "time": 1734518400,
  "message": "request"
}
```

#### Database error

```json
{
  "level": "error",
  "error": "connection timeout",
  "time": 1734518400,
  "message": "failed to acquire pg connection"
}
```

## Searching logs

### Find errors in last hour

```bash
aws logs filter-log-events \\\n  --log-group-name /ecs/staging/address-api \\\n  --filter-pattern '{ $.level = "error" }' \\\n  --start-time $(date -u -d '1 hour ago' +%s)000 \\\n  --region ap-south-1
```

### Find specific endpoint errors

```bash
aws logs filter-log-events \\\n  --log-group-name /ecs/staging/address-api \\\n  --filter-pattern '{ $.path = "/api/v1/geoencode" && $.status >= 500 }' \\\n  --region ap-south-1
```

### Find slow requests

```bash
aws logs filter-log-events \\\n  --log-group-name /ecs/staging/address-api \\\n  --filter-pattern '{ $.duration_ms > 1000 }' \\\n  --region ap-south-1
```

## ECS debugging

### Check service status

```bash
aws ecs describe-services \\\n  --cluster staging-ecs-cluster \\\n  --services address-api \\\n  --region ap-south-1 \\\n  --query 'services[0].{status:status,runningCount:runningCount,desiredCount:desiredCount,deployments:deployments}'
```

### List running tasks

```bash
aws ecs list-tasks \\\n  --cluster staging-ecs-cluster \\\n  --service-name address-api \\\n  --region ap-south-1
```

### Describe task

```bash
aws ecs describe-tasks \\\n  --cluster staging-ecs-cluster \\\n  --tasks <task-arn> \\\n  --region ap-south-1
```

### Check stopped tasks

```bash
aws ecs list-tasks \\\n  --cluster staging-ecs-cluster \\\n  --service-name address-api \\\n  --desired-status STOPPED \\\n  --region ap-south-1
```

### View stopped task reason

```bash
aws ecs describe-tasks \\\n  --cluster staging-ecs-cluster \\\n  --tasks <stopped-task-arn> \\\n  --region ap-south-1 \\\n  --query 'tasks[0].{stoppedReason:stoppedReason,containers:containers[0].{reason:reason,exitCode:exitCode}}'
```

## ECS Exec (SSH into container)

ECS Exec allows you to execute commands inside running containers.

### Prerequisites

- ECS Exec must be enabled (it is by default)
- Session Manager plugin installed

### Connect to container

```bash
# Get task ID
TASK_ID=$(aws ecs list-tasks \\\n  --cluster staging-ecs-cluster \\\n  --service-name address-api \\\n  --region ap-south-1 \\\n  --query 'taskArns[0]' \\\n  --output text | cut -d'/' -f3)

# Execute shell
aws ecs execute-command \\\n  --cluster staging-ecs-cluster \\\n  --task $TASK_ID \\\n  --container address-api \\\n  --interactive \\\n  --command "/bin/sh" \\\n  --region ap-south-1
```

### Useful commands inside container

```bash
# Check environment variables
env | grep RDS

# Check if database is reachable
nc -zv $RDS_HOSTNAME $RDS_PORT

# Check disk space
df -h

# Check memory usage
free -m

# View process list
ps aux

# Check network connections
netstat -an | grep ESTABLISHED
```

## ALB debugging

### Check target health

```bash
# Get target group ARN
TG_ARN=$(aws elbv2 describe-target-groups \\\n  --names staging-address-api-tg \\\n  --region ap-south-1 \\\n  --query 'TargetGroups[0].TargetGroupArn' \\\n  --output text)

# Check target health
aws elbv2 describe-target-health \\\n  --target-group-arn $TG_ARN \\\n  --region ap-south-1
```

### View ALB access logs

ALB access logs are stored in S3 (if enabled):

```bash
aws s3 ls s3://staging-alb-logs/address-api/ --recursive
```

## Database debugging

See [Database access](/engine/address-api/operations/database-access) for connecting to the database.

### Check active queries

```sql
SELECT
  pid,
  now() - query_start AS duration,
  state,
  query
FROM pg_stat_activity
WHERE datname = 'address_api'
  AND state != 'idle'
ORDER BY duration DESC;
```

### Check slow queries

```sql
SELECT
  calls,
  total_exec_time,
  mean_exec_time,
  query
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

### Check locks

```sql
SELECT
  locktype,
  relation::regclass,
  mode,
  granted,
  pid
FROM pg_locks
WHERE NOT granted;
```

## Common issues

### High CPU usage

**Symptoms**: ECS tasks using >80% CPU

**Debug**:
1. Check CloudWatch metrics for CPU utilization
2. Check logs for slow queries or infinite loops
3. Use ECS Exec to check running processes

**Fix**:
- Optimize slow queries
- Add database indexes
- Scale up task CPU allocation

### High memory usage

**Symptoms**: ECS tasks using >80% memory

**Debug**:
1. Check CloudWatch metrics for memory utilization
2. Check for memory leaks in logs
3. Use ECS Exec to check memory usage

**Fix**:
- Fix memory leaks
- Scale up task memory allocation
- Add connection pool limits

### Database connection errors

**Symptoms**: "could not acquire connection" errors

**Debug**:
1. Check database connection pool metrics in health endpoint
2. Check RDS metrics for connection count
3. Check for long-running queries

**Fix**:
- Increase connection pool size
- Optimize slow queries
- Scale up RDS instance

### Google API errors

**Symptoms**: Geocoding requests failing

**Debug**:
1. Check logs for Google API errors
2. Verify API key is valid
3. Check Google Cloud Console for quota limits

**Fix**:
- Verify API key in Secrets Manager
- Increase Google API quota
- Implement retry logic

## Performance profiling

### Enable Go profiling

Add pprof endpoints (for development only):

```go
import _ "net/http/pprof"

// In main.go
go func() {
    log.Println(http.ListenAndServe("localhost:6060", nil))
}()
```

### Collect CPU profile

```bash
# From inside container
curl http://localhost:6060/debug/pprof/profile?seconds=30 > cpu.prof

# Analyze
go tool pprof cpu.prof
```

### Collect memory profile

```bash
curl http://localhost:6060/debug/pprof/heap > mem.prof
go tool pprof mem.prof
```

## Next steps

- [Monitoring](/engine/address-api/operations/monitoring)
- [Troubleshooting](/engine/address-api/operations/troubleshooting)
- [Database access](/engine/address-api/operations/database-access)
