---
title: Monitoring and alerts
description: How the Address API is monitored and how alerts are configured
---

This guide explains the monitoring and alerting setup for the Address API.

## Monitoring tools

The Address API uses two primary monitoring tools:

1. **AWS CloudWatch**: Infrastructure and application metrics
2. **Better Stack**: External monitoring and alerting

## Better Stack

**TODO: Provide Better Stack details**

Better Stack provides external monitoring and alerting for the Address API.

### Metrics tracked

The following metrics should be tracked in Better Stack:

- **API call volume**: Total number of requests per minute/hour
- **Status code distribution**: Count of 2xx, 4xx, 5xx responses
- **Response times**: P50, P95, P99 latencies
- **Error rate**: Percentage of failed requests
- **Availability**: Uptime percentage

### Alerts configured

**TODO: Document current alert configuration**

Recommended alerts:

| Alert | Condition | Threshold | Action |
|-------|-----------|-----------|--------|
| High error rate | 5xx errors > threshold | 5% over 5 minutes | Notify on-call engineer |
| Service down | Health check fails | 3 consecutive failures | Page on-call engineer |
| Slow responses | P95 latency > threshold | 2000ms over 5 minutes | Notify team |
| High traffic | Requests > threshold | 1000 req/min | Notify team |

### Dashboard

**TODO: Provide Better Stack dashboard URL**

The Better Stack dashboard should show:

- Real-time request rate
- Status code distribution
- Response time percentiles
- Error rate trends
- Availability uptime

## CloudWatch metrics

### Application metrics

| Metric | Description | Unit |
|--------|-------------|------|
| `RequestCount` | Total number of requests | Count |
| `2xxCount` | Successful requests | Count |
| `4xxCount` | Client errors | Count |
| `5xxCount` | Server errors | Count |
| `ResponseTime` | Request duration | Milliseconds |

### ECS metrics

| Metric | Description | Threshold |
|--------|-------------|-----------|
| `CPUUtilization` | Task CPU usage | < 80% |
| `MemoryUtilization` | Task memory usage | < 80% |
| `TaskCount` | Number of running tasks | >= 2 |

### RDS metrics

| Metric | Description | Threshold |
|--------|-------------|-----------|
| `CPUUtilization` | Database CPU usage | < 80% |
| `DatabaseConnections` | Active connections | < 40 (out of 48 max) |
| `FreeableMemory` | Available memory | > 1GB |
| `ReadLatency` | Query read latency | < 10ms |
| `WriteLatency` | Query write latency | < 10ms |

## Health checks

### Application health check

Endpoint: `GET /healthz`

Returns:

```json
{
  "api": true,
  "database_pool": {
    "total_connections": 12,
    "acquired_connections": 2,
    "idle_connections": 10,
    "max_connections": 48,
    "acquire_duration_ms": 2
  }
}
```

### ALB health check

- **Path**: `/healthz`
- **Interval**: 30 seconds
- **Timeout**: 5 seconds
- **Healthy threshold**: 2 consecutive successes
- **Unhealthy threshold**: 3 consecutive failures

## Alert channels

**TODO: Document alert channels**

Alerts should be sent to:

- **Slack**: #alerts-address-api channel
- **PagerDuty**: For critical production alerts
- **Email**: Team distribution list

## Incident response

When an alert fires:

1. **Acknowledge**: Acknowledge the alert in Better Stack/PagerDuty
2. **Assess**: Check CloudWatch logs and metrics
3. **Diagnose**: Identify the root cause
4. **Mitigate**: Take action to resolve the issue
5. **Document**: Record the incident and resolution
6. **Follow-up**: Create tasks to prevent recurrence

## Next steps

- [Debugging](/engine/address-api/operations/debugging)
- [Common failures](/engine/address-api/operations/common-failures)
- [Database connection](/engine/address-api/operations/database-connection)
